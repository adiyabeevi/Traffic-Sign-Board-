{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# **Title: Traffic Sign Classification Using Deep Learning**\n",
        "\n",
        "## **Introduction:**\n",
        "\n",
        "Road safety is a critical aspect of modern transportation. Drivers and autonomous vehicles must correctly recognize traffic signs to prevent accidents and ensure smooth traffic flow. Manual recognition or poor detection systems can lead to misinterpretation, resulting in dangerous situations.\n",
        "\n",
        "This project focuses on building a deep learning-based traffic sign classifier using a Convolutional Neural Network (CNN). The model is trained to automatically identify and categorize traffic signs from images. By leveraging a large labeled dataset (GTSRB), image preprocessing, and data augmentation techniques, the model learns to recognize patterns, shapes, and colors of different traffic signs.\n",
        "\n",
        "## **Goals of the Project:**\n",
        "\n",
        "Develop a CNN model capable of classifying 43 types of traffic signs.\n",
        "\n",
        "Achieve high classification accuracy (target ≥ 90%) on unseen validation data.\n",
        "\n",
        "## **Expected Outcomes:**\n",
        "\n",
        "A trained CNN model that can accurately predict traffic signs from images.\n"
      ],
      "metadata": {
        "id": "a1yc1ZmrH9Ll"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ah0LMIob4LT5"
      },
      "outputs": [],
      "source": [
        "# Import necessary libraries\n",
        "\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import cv2\n",
        "import os\n",
        "import matplotlib.pyplot as plt\n",
        "from tensorflow.keras.utils import to_categorical\n",
        "from tensorflow.keras.preprocessing.image import load_img, img_to_array  # For loading and converting images\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import classification_report, confusion_matrix\n",
        "import seaborn as sns\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install kaggle"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ktPezJEUM5pD",
        "outputId": "124a5fa8-1d6a-45c1-e44d-0e1b7b211248"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: kaggle in /usr/local/lib/python3.12/dist-packages (1.7.4.5)\n",
            "Requirement already satisfied: bleach in /usr/local/lib/python3.12/dist-packages (from kaggle) (6.2.0)\n",
            "Requirement already satisfied: certifi>=14.05.14 in /usr/local/lib/python3.12/dist-packages (from kaggle) (2025.10.5)\n",
            "Requirement already satisfied: charset-normalizer in /usr/local/lib/python3.12/dist-packages (from kaggle) (3.4.3)\n",
            "Requirement already satisfied: idna in /usr/local/lib/python3.12/dist-packages (from kaggle) (3.10)\n",
            "Requirement already satisfied: protobuf in /usr/local/lib/python3.12/dist-packages (from kaggle) (5.29.5)\n",
            "Requirement already satisfied: python-dateutil>=2.5.3 in /usr/local/lib/python3.12/dist-packages (from kaggle) (2.9.0.post0)\n",
            "Requirement already satisfied: python-slugify in /usr/local/lib/python3.12/dist-packages (from kaggle) (8.0.4)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.12/dist-packages (from kaggle) (2.32.4)\n",
            "Requirement already satisfied: setuptools>=21.0.0 in /usr/local/lib/python3.12/dist-packages (from kaggle) (75.2.0)\n",
            "Requirement already satisfied: six>=1.10 in /usr/local/lib/python3.12/dist-packages (from kaggle) (1.17.0)\n",
            "Requirement already satisfied: text-unidecode in /usr/local/lib/python3.12/dist-packages (from kaggle) (1.3)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.12/dist-packages (from kaggle) (4.67.1)\n",
            "Requirement already satisfied: urllib3>=1.15.1 in /usr/local/lib/python3.12/dist-packages (from kaggle) (2.5.0)\n",
            "Requirement already satisfied: webencodings in /usr/local/lib/python3.12/dist-packages (from kaggle) (0.5.1)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import files\n",
        "files.upload()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 95
        },
        "id": "v7ZmDkX-KxhI",
        "outputId": "1ce0433a-8fb4-48c4-9b16-61485f5e8896"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "     <input type=\"file\" id=\"files-c3d0ec3e-5ef8-4e8f-a8bf-d4e7f51fc308\" name=\"files[]\" multiple disabled\n",
              "        style=\"border:none\" />\n",
              "     <output id=\"result-c3d0ec3e-5ef8-4e8f-a8bf-d4e7f51fc308\">\n",
              "      Upload widget is only available when the cell has been executed in the\n",
              "      current browser session. Please rerun this cell to enable.\n",
              "      </output>\n",
              "      <script>// Copyright 2017 Google LLC\n",
              "//\n",
              "// Licensed under the Apache License, Version 2.0 (the \"License\");\n",
              "// you may not use this file except in compliance with the License.\n",
              "// You may obtain a copy of the License at\n",
              "//\n",
              "//      http://www.apache.org/licenses/LICENSE-2.0\n",
              "//\n",
              "// Unless required by applicable law or agreed to in writing, software\n",
              "// distributed under the License is distributed on an \"AS IS\" BASIS,\n",
              "// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
              "// See the License for the specific language governing permissions and\n",
              "// limitations under the License.\n",
              "\n",
              "/**\n",
              " * @fileoverview Helpers for google.colab Python module.\n",
              " */\n",
              "(function(scope) {\n",
              "function span(text, styleAttributes = {}) {\n",
              "  const element = document.createElement('span');\n",
              "  element.textContent = text;\n",
              "  for (const key of Object.keys(styleAttributes)) {\n",
              "    element.style[key] = styleAttributes[key];\n",
              "  }\n",
              "  return element;\n",
              "}\n",
              "\n",
              "// Max number of bytes which will be uploaded at a time.\n",
              "const MAX_PAYLOAD_SIZE = 100 * 1024;\n",
              "\n",
              "function _uploadFiles(inputId, outputId) {\n",
              "  const steps = uploadFilesStep(inputId, outputId);\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  // Cache steps on the outputElement to make it available for the next call\n",
              "  // to uploadFilesContinue from Python.\n",
              "  outputElement.steps = steps;\n",
              "\n",
              "  return _uploadFilesContinue(outputId);\n",
              "}\n",
              "\n",
              "// This is roughly an async generator (not supported in the browser yet),\n",
              "// where there are multiple asynchronous steps and the Python side is going\n",
              "// to poll for completion of each step.\n",
              "// This uses a Promise to block the python side on completion of each step,\n",
              "// then passes the result of the previous step as the input to the next step.\n",
              "function _uploadFilesContinue(outputId) {\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  const steps = outputElement.steps;\n",
              "\n",
              "  const next = steps.next(outputElement.lastPromiseValue);\n",
              "  return Promise.resolve(next.value.promise).then((value) => {\n",
              "    // Cache the last promise value to make it available to the next\n",
              "    // step of the generator.\n",
              "    outputElement.lastPromiseValue = value;\n",
              "    return next.value.response;\n",
              "  });\n",
              "}\n",
              "\n",
              "/**\n",
              " * Generator function which is called between each async step of the upload\n",
              " * process.\n",
              " * @param {string} inputId Element ID of the input file picker element.\n",
              " * @param {string} outputId Element ID of the output display.\n",
              " * @return {!Iterable<!Object>} Iterable of next steps.\n",
              " */\n",
              "function* uploadFilesStep(inputId, outputId) {\n",
              "  const inputElement = document.getElementById(inputId);\n",
              "  inputElement.disabled = false;\n",
              "\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  outputElement.innerHTML = '';\n",
              "\n",
              "  const pickedPromise = new Promise((resolve) => {\n",
              "    inputElement.addEventListener('change', (e) => {\n",
              "      resolve(e.target.files);\n",
              "    });\n",
              "  });\n",
              "\n",
              "  const cancel = document.createElement('button');\n",
              "  inputElement.parentElement.appendChild(cancel);\n",
              "  cancel.textContent = 'Cancel upload';\n",
              "  const cancelPromise = new Promise((resolve) => {\n",
              "    cancel.onclick = () => {\n",
              "      resolve(null);\n",
              "    };\n",
              "  });\n",
              "\n",
              "  // Wait for the user to pick the files.\n",
              "  const files = yield {\n",
              "    promise: Promise.race([pickedPromise, cancelPromise]),\n",
              "    response: {\n",
              "      action: 'starting',\n",
              "    }\n",
              "  };\n",
              "\n",
              "  cancel.remove();\n",
              "\n",
              "  // Disable the input element since further picks are not allowed.\n",
              "  inputElement.disabled = true;\n",
              "\n",
              "  if (!files) {\n",
              "    return {\n",
              "      response: {\n",
              "        action: 'complete',\n",
              "      }\n",
              "    };\n",
              "  }\n",
              "\n",
              "  for (const file of files) {\n",
              "    const li = document.createElement('li');\n",
              "    li.append(span(file.name, {fontWeight: 'bold'}));\n",
              "    li.append(span(\n",
              "        `(${file.type || 'n/a'}) - ${file.size} bytes, ` +\n",
              "        `last modified: ${\n",
              "            file.lastModifiedDate ? file.lastModifiedDate.toLocaleDateString() :\n",
              "                                    'n/a'} - `));\n",
              "    const percent = span('0% done');\n",
              "    li.appendChild(percent);\n",
              "\n",
              "    outputElement.appendChild(li);\n",
              "\n",
              "    const fileDataPromise = new Promise((resolve) => {\n",
              "      const reader = new FileReader();\n",
              "      reader.onload = (e) => {\n",
              "        resolve(e.target.result);\n",
              "      };\n",
              "      reader.readAsArrayBuffer(file);\n",
              "    });\n",
              "    // Wait for the data to be ready.\n",
              "    let fileData = yield {\n",
              "      promise: fileDataPromise,\n",
              "      response: {\n",
              "        action: 'continue',\n",
              "      }\n",
              "    };\n",
              "\n",
              "    // Use a chunked sending to avoid message size limits. See b/62115660.\n",
              "    let position = 0;\n",
              "    do {\n",
              "      const length = Math.min(fileData.byteLength - position, MAX_PAYLOAD_SIZE);\n",
              "      const chunk = new Uint8Array(fileData, position, length);\n",
              "      position += length;\n",
              "\n",
              "      const base64 = btoa(String.fromCharCode.apply(null, chunk));\n",
              "      yield {\n",
              "        response: {\n",
              "          action: 'append',\n",
              "          file: file.name,\n",
              "          data: base64,\n",
              "        },\n",
              "      };\n",
              "\n",
              "      let percentDone = fileData.byteLength === 0 ?\n",
              "          100 :\n",
              "          Math.round((position / fileData.byteLength) * 100);\n",
              "      percent.textContent = `${percentDone}% done`;\n",
              "\n",
              "    } while (position < fileData.byteLength);\n",
              "  }\n",
              "\n",
              "  // All done.\n",
              "  yield {\n",
              "    response: {\n",
              "      action: 'complete',\n",
              "    }\n",
              "  };\n",
              "}\n",
              "\n",
              "scope.google = scope.google || {};\n",
              "scope.google.colab = scope.google.colab || {};\n",
              "scope.google.colab._files = {\n",
              "  _uploadFiles,\n",
              "  _uploadFilesContinue,\n",
              "};\n",
              "})(self);\n",
              "</script> "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Saving kaggle.json to kaggle (1).json\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'kaggle (1).json': b'{\"username\":\"adiyasalim\",\"key\":\"8aa9104806ff671cea28aac1f2c9a305\"}'}"
            ]
          },
          "metadata": {},
          "execution_count": 1
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!mkdir -p ~/.kaggle\n",
        "!cp kaggle.json ~/.kaggle/\n",
        "!chmod 600 ~/.kaggle/kaggle.json\n"
      ],
      "metadata": {
        "id": "ll4KifvaKCgS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!kaggle datasets download -d meowmeowmeowmeowmeow/gtsrb-german-traffic-sign\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TiEcBBo5L6fb",
        "outputId": "ad62b32f-258c-4db4-d033-83a3de4a5f3e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Dataset URL: https://www.kaggle.com/datasets/meowmeowmeowmeowmeow/gtsrb-german-traffic-sign\n",
            "License(s): CC0-1.0\n",
            "Downloading gtsrb-german-traffic-sign.zip to /content\n",
            " 97% 595M/612M [00:01<00:00, 304MB/s]\n",
            "100% 612M/612M [00:01<00:00, 406MB/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Extracting the data\n",
        "import zipfile\n",
        "\n",
        "with zipfile.ZipFile(\"gtsrb-german-traffic-sign.zip\", 'r') as zip_ref:\n",
        "    zip_ref.extractall(\"GTSRB\")\n"
      ],
      "metadata": {
        "id": "0gZtcNXfMA4y"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "# Load the CSV files\n",
        "train_csv_path = \"/content/GTSRB/Train.csv\"\n",
        "test_csv_path = \"/content/GTSRB/Test.csv\"\n",
        "\n",
        "train_df = pd.read_csv(train_csv_path)\n",
        "test_df = pd.read_csv(test_csv_path)\n",
        "\n",
        "print(\"Training samples:\", len(train_df))\n",
        "print(\"Test samples:\", len(test_df))\n",
        "print(\"Number of classes:\", train_df['ClassId'].nunique())\n",
        "\n",
        "# Check the structure\n",
        "print(\"\\nTraining data columns:\", train_df.columns.tolist())\n",
        "print(\"Test data columns:\", test_df.columns.tolist())\n",
        "print(\"\\nFirst few rows of training data:\")\n",
        "print(train_df.head())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UX_1Gvbks9q_",
        "outputId": "8dec558a-0138-405e-8bbd-4900cadfb3c7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training samples: 39209\n",
            "Test samples: 12630\n",
            "Number of classes: 43\n",
            "\n",
            "Training data columns: ['Width', 'Height', 'Roi.X1', 'Roi.Y1', 'Roi.X2', 'Roi.Y2', 'ClassId', 'Path']\n",
            "Test data columns: ['Width', 'Height', 'Roi.X1', 'Roi.Y1', 'Roi.X2', 'Roi.Y2', 'ClassId', 'Path']\n",
            "\n",
            "First few rows of training data:\n",
            "   Width  Height  Roi.X1  Roi.Y1  Roi.X2  Roi.Y2  ClassId  \\\n",
            "0     27      26       5       5      22      20       20   \n",
            "1     28      27       5       6      23      22       20   \n",
            "2     29      26       6       5      24      21       20   \n",
            "3     28      27       5       6      23      22       20   \n",
            "4     28      26       5       5      23      21       20   \n",
            "\n",
            "                             Path  \n",
            "0  Train/20/00020_00000_00000.png  \n",
            "1  Train/20/00020_00000_00001.png  \n",
            "2  Train/20/00020_00000_00002.png  \n",
            "3  Train/20/00020_00000_00003.png  \n",
            "4  Train/20/00020_00000_00004.png  \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Listing the directories within the GTSRB folder\n",
        "os.listdir(\"GTSRB\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cJD-g-EYMHNt",
        "outputId": "1e4882c4-1b59-4c6e-c9a9-8aff99b7843e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['test',\n",
              " 'Meta',\n",
              " 'Meta.csv',\n",
              " 'meta',\n",
              " 'Test.csv',\n",
              " 'train',\n",
              " 'Train.csv',\n",
              " 'Train',\n",
              " 'Test']"
            ]
          },
          "metadata": {},
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.image as mpimg\n",
        "\n",
        "# Path to a sample image\n",
        "sample_image_path = \"/content/GTSRB/Train/0/00000_00000_00009.png\"\n",
        "\n",
        "# Load and display the image\n",
        "img = mpimg.imread(sample_image_path)\n",
        "plt.imshow(img)\n",
        "plt.axis('off')  # Hide axes\n",
        "plt.show()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 406
        },
        "id": "fVTf-vM_MK_G",
        "outputId": "79b19ac2-7fb3-4b50-8fbe-121fe945d27d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Data Pre processing\n",
        "\n",
        "Resize all images to a consistent shape (e.g., 64×64 pixels).\n",
        "\n",
        "Normalize pixel values (scale to 0–1).\n",
        "\n",
        "One-hot encode labels for multi-class classification.\n",
        "\n",
        "Split the dataset into training (80%) and validation (20%)."
      ],
      "metadata": {
        "id": "AcFgABWWNUXb"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "EgcsNek217Jk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import cv2\n",
        "from tensorflow.keras.utils import to_categorical\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "# Parameters\n",
        "img_size = 64\n",
        "\n",
        "def preprocess_data(df, dataset_path):\n",
        "    images = []\n",
        "    labels = []\n",
        "\n",
        "    for idx, row in df.iterrows():\n",
        "\n",
        "        img_path = os.path.join(dataset_path, row['Path'])\n",
        "\n",
        "        # Load and preprocess image\n",
        "        img = cv2.imread(img_path)\n",
        "        if img is not None:\n",
        "            # Convert BGR to RGB (OpenCV loads as BGR)\n",
        "            img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
        "            img = cv2.resize(img, (img_size, img_size))\n",
        "            images.append(img)\n",
        "            labels.append(row['ClassId'])\n",
        "\n",
        "    # Convert to numpy arrays\n",
        "    images = np.array(images, dtype='float32') / 255.0\n",
        "    labels = np.array(labels)\n",
        "\n",
        "    return images, labels\n",
        "\n",
        "# Preprocess training data\n",
        "X_train, y_train = preprocess_data(train_df, \"/content/GTSRB/\")\n",
        "\n",
        "# One-hot encode labels\n",
        "num_classes = len(np.unique(y_train))\n",
        "y_train = to_categorical(y_train, num_classes=num_classes)\n",
        "\n",
        "# Train-validation split\n",
        "X_train, X_val, y_train, y_val = train_test_split(X_train, y_train, test_size=0.2, random_state=42)\n",
        "\n",
        "print(\"Training samples:\", X_train.shape[0])\n",
        "print(\"Validation samples:\", X_val.shape[0])\n",
        "print(\"Number of classes:\", num_classes)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZtAKszkcNam3",
        "outputId": "68f4658c-29fd-4f7c-9f39-6d981344951b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training samples: 31367\n",
            "Validation samples: 7842\n",
            "Number of classes: 43\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Data Augmentation\n",
        "This will slightly modify training images during training, improving model robustness."
      ],
      "metadata": {
        "id": "4VhN-zUrPmmt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
        "\n",
        "datagen = ImageDataGenerator(\n",
        "    rotation_range=15,\n",
        "    width_shift_range=0.1,\n",
        "    height_shift_range=0.1,\n",
        "    zoom_range=0.1\n",
        ")\n",
        "\n",
        "datagen.fit(X_train)\n"
      ],
      "metadata": {
        "id": "gVgkLhByOCIm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Step 3 : Build and train the CNN Model**\n",
        "\n"
      ],
      "metadata": {
        "id": "wknhEwi-Oznu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Conv2D, MaxPooling2D, Flatten, Dense, Dropout\n",
        "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
        "\n",
        "# Build model\n",
        "model = Sequential([\n",
        "    Conv2D(32, (3,3), activation='relu', input_shape=(64,64,3)),\n",
        "    MaxPooling2D((2,2)),\n",
        "    Conv2D(64, (3,3), activation='relu'),\n",
        "    MaxPooling2D((2,2)),\n",
        "    Conv2D(128, (3,3), activation='relu'),\n",
        "    MaxPooling2D((2,2)),\n",
        "    Flatten(),\n",
        "    Dense(128, activation='relu'),\n",
        "    Dropout(0.5),\n",
        "    Dense(num_classes, activation='softmax')\n",
        "])\n",
        "\n",
        "# Compile model\n",
        "model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
        "model.summary()\n",
        "\n",
        "# Data augmentation\n",
        "datagen = ImageDataGenerator(\n",
        "    rotation_range=15,\n",
        "    width_shift_range=0.1,\n",
        "    height_shift_range=0.1,\n",
        "    zoom_range=0.1\n",
        ")\n",
        "datagen.fit(X_train)\n",
        "\n",
        "# Train the model\n",
        "history = model.fit(\n",
        "    datagen.flow(X_train, y_train, batch_size=64),\n",
        "    validation_data=(X_val, y_val),\n",
        "    epochs=40\n",
        ")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 874
        },
        "id": "ZeJYB6dlOz5s",
        "outputId": "8f99ea88-ebc4-4484-f3fb-5624d4c3fca3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1mModel: \"sequential_1\"\u001b[0m\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"sequential_1\"</span>\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
              "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                   \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape          \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m      Param #\u001b[0m\u001b[1m \u001b[0m┃\n",
              "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
              "│ conv2d_3 (\u001b[38;5;33mConv2D\u001b[0m)               │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m62\u001b[0m, \u001b[38;5;34m62\u001b[0m, \u001b[38;5;34m32\u001b[0m)     │           \u001b[38;5;34m896\u001b[0m │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ max_pooling2d_3 (\u001b[38;5;33mMaxPooling2D\u001b[0m)  │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m31\u001b[0m, \u001b[38;5;34m31\u001b[0m, \u001b[38;5;34m32\u001b[0m)     │             \u001b[38;5;34m0\u001b[0m │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ conv2d_4 (\u001b[38;5;33mConv2D\u001b[0m)               │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m29\u001b[0m, \u001b[38;5;34m29\u001b[0m, \u001b[38;5;34m64\u001b[0m)     │        \u001b[38;5;34m18,496\u001b[0m │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ max_pooling2d_4 (\u001b[38;5;33mMaxPooling2D\u001b[0m)  │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m14\u001b[0m, \u001b[38;5;34m14\u001b[0m, \u001b[38;5;34m64\u001b[0m)     │             \u001b[38;5;34m0\u001b[0m │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ conv2d_5 (\u001b[38;5;33mConv2D\u001b[0m)               │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m12\u001b[0m, \u001b[38;5;34m12\u001b[0m, \u001b[38;5;34m128\u001b[0m)    │        \u001b[38;5;34m73,856\u001b[0m │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ max_pooling2d_5 (\u001b[38;5;33mMaxPooling2D\u001b[0m)  │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m6\u001b[0m, \u001b[38;5;34m6\u001b[0m, \u001b[38;5;34m128\u001b[0m)      │             \u001b[38;5;34m0\u001b[0m │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ flatten_1 (\u001b[38;5;33mFlatten\u001b[0m)             │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m4608\u001b[0m)           │             \u001b[38;5;34m0\u001b[0m │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ dense_2 (\u001b[38;5;33mDense\u001b[0m)                 │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m128\u001b[0m)            │       \u001b[38;5;34m589,952\u001b[0m │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ dropout_1 (\u001b[38;5;33mDropout\u001b[0m)             │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m128\u001b[0m)            │             \u001b[38;5;34m0\u001b[0m │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ dense_3 (\u001b[38;5;33mDense\u001b[0m)                 │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m43\u001b[0m)             │         \u001b[38;5;34m5,547\u001b[0m │\n",
              "└─────────────────────────────────┴────────────────────────┴───────────────┘\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
              "┃<span style=\"font-weight: bold\"> Layer (type)                    </span>┃<span style=\"font-weight: bold\"> Output Shape           </span>┃<span style=\"font-weight: bold\">       Param # </span>┃\n",
              "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
              "│ conv2d_3 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv2D</span>)               │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">62</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">62</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>)     │           <span style=\"color: #00af00; text-decoration-color: #00af00\">896</span> │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ max_pooling2d_3 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">MaxPooling2D</span>)  │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">31</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">31</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>)     │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ conv2d_4 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv2D</span>)               │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">29</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">29</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)     │        <span style=\"color: #00af00; text-decoration-color: #00af00\">18,496</span> │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ max_pooling2d_4 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">MaxPooling2D</span>)  │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">14</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">14</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)     │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ conv2d_5 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv2D</span>)               │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">12</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">12</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)    │        <span style=\"color: #00af00; text-decoration-color: #00af00\">73,856</span> │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ max_pooling2d_5 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">MaxPooling2D</span>)  │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">6</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">6</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)      │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ flatten_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Flatten</span>)             │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">4608</span>)           │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ dense_2 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                 │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)            │       <span style=\"color: #00af00; text-decoration-color: #00af00\">589,952</span> │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ dropout_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)             │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)            │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ dense_3 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                 │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">43</span>)             │         <span style=\"color: #00af00; text-decoration-color: #00af00\">5,547</span> │\n",
              "└─────────────────────────────────┴────────────────────────┴───────────────┘\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m688,747\u001b[0m (2.63 MB)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">688,747</span> (2.63 MB)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m688,747\u001b[0m (2.63 MB)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">688,747</span> (2.63 MB)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/40\n",
            "\u001b[1m 12/491\u001b[0m \u001b[37m━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m4:07\u001b[0m 517ms/step - accuracy: 0.0352 - loss: 3.7163"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-1495896733.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     31\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     32\u001b[0m \u001b[0;31m# Train the model\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 33\u001b[0;31m history = model.fit(\n\u001b[0m\u001b[1;32m     34\u001b[0m     \u001b[0mdatagen\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mflow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m64\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     35\u001b[0m     \u001b[0mvalidation_data\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_val\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_val\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/keras/src/utils/traceback_utils.py\u001b[0m in \u001b[0;36merror_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    115\u001b[0m         \u001b[0mfiltered_tb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    116\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 117\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    118\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    119\u001b[0m             \u001b[0mfiltered_tb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_process_traceback_frames\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__traceback__\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/keras/src/backend/tensorflow/trainer.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq)\u001b[0m\n\u001b[1;32m    375\u001b[0m                 \u001b[0;32mfor\u001b[0m \u001b[0mstep\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0miterator\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mepoch_iterator\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    376\u001b[0m                     \u001b[0mcallbacks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_train_batch_begin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 377\u001b[0;31m                     \u001b[0mlogs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    378\u001b[0m                     \u001b[0mcallbacks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_train_batch_end\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlogs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    379\u001b[0m                     \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstop_training\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/keras/src/backend/tensorflow/trainer.py\u001b[0m in \u001b[0;36mfunction\u001b[0;34m(iterator)\u001b[0m\n\u001b[1;32m    218\u001b[0m                 \u001b[0miterator\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mIterator\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdistribute\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mDistributedIterator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    219\u001b[0m             ):\n\u001b[0;32m--> 220\u001b[0;31m                 \u001b[0mopt_outputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmulti_step_on_iterator\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    221\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mopt_outputs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhas_value\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    222\u001b[0m                     \u001b[0;32mraise\u001b[0m \u001b[0mStopIteration\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/tensorflow/python/util/traceback_utils.py\u001b[0m in \u001b[0;36merror_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    148\u001b[0m     \u001b[0mfiltered_tb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    149\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 150\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    151\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    152\u001b[0m       \u001b[0mfiltered_tb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_process_traceback_frames\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__traceback__\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/tensorflow/python/eager/polymorphic_function/polymorphic_function.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    831\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    832\u001b[0m       \u001b[0;32mwith\u001b[0m \u001b[0mOptionalXlaContext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jit_compile\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 833\u001b[0;31m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    834\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    835\u001b[0m       \u001b[0mnew_tracing_count\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexperimental_get_tracing_count\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/tensorflow/python/eager/polymorphic_function/polymorphic_function.py\u001b[0m in \u001b[0;36m_call\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    876\u001b[0m       \u001b[0;31m# In this case we have not created variables on the first call. So we can\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    877\u001b[0m       \u001b[0;31m# run the first trace but we should fail if variables are created.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 878\u001b[0;31m       results = tracing_compilation.call_function(\n\u001b[0m\u001b[1;32m    879\u001b[0m           \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_variable_creation_config\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    880\u001b[0m       )\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/tensorflow/python/eager/polymorphic_function/tracing_compilation.py\u001b[0m in \u001b[0;36mcall_function\u001b[0;34m(args, kwargs, tracing_options)\u001b[0m\n\u001b[1;32m    137\u001b[0m   \u001b[0mbound_args\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfunction\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfunction_type\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbind\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    138\u001b[0m   \u001b[0mflat_inputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfunction\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfunction_type\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munpack_inputs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbound_args\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 139\u001b[0;31m   return function._call_flat(  # pylint: disable=protected-access\n\u001b[0m\u001b[1;32m    140\u001b[0m       \u001b[0mflat_inputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcaptured_inputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfunction\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcaptured_inputs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    141\u001b[0m   )\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/tensorflow/python/eager/polymorphic_function/concrete_function.py\u001b[0m in \u001b[0;36m_call_flat\u001b[0;34m(self, tensor_inputs, captured_inputs)\u001b[0m\n\u001b[1;32m   1320\u001b[0m         and executing_eagerly):\n\u001b[1;32m   1321\u001b[0m       \u001b[0;31m# No tape is watching; skip to running the function.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1322\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_inference_function\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcall_preflattened\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1323\u001b[0m     forward_backward = self._select_forward_and_backward_functions(\n\u001b[1;32m   1324\u001b[0m         \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/tensorflow/python/eager/polymorphic_function/atomic_function.py\u001b[0m in \u001b[0;36mcall_preflattened\u001b[0;34m(self, args)\u001b[0m\n\u001b[1;32m    214\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0mcall_preflattened\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mSequence\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mcore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTensor\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mAny\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    215\u001b[0m     \u001b[0;34m\"\"\"Calls with flattened tensor inputs and returns the structured output.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 216\u001b[0;31m     \u001b[0mflat_outputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcall_flat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    217\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfunction_type\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpack_output\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mflat_outputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    218\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/tensorflow/python/eager/polymorphic_function/atomic_function.py\u001b[0m in \u001b[0;36mcall_flat\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m    249\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mrecord\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstop_recording\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    250\u001b[0m           \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_bound_context\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexecuting_eagerly\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 251\u001b[0;31m             outputs = self._bound_context.call_function(\n\u001b[0m\u001b[1;32m    252\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    253\u001b[0m                 \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/tensorflow/python/eager/context.py\u001b[0m in \u001b[0;36mcall_function\u001b[0;34m(self, name, tensor_inputs, num_outputs)\u001b[0m\n\u001b[1;32m   1686\u001b[0m     \u001b[0mcancellation_context\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcancellation\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcontext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1687\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mcancellation_context\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1688\u001b[0;31m       outputs = execute.execute(\n\u001b[0m\u001b[1;32m   1689\u001b[0m           \u001b[0mname\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdecode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"utf-8\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1690\u001b[0m           \u001b[0mnum_outputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mnum_outputs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/tensorflow/python/eager/execute.py\u001b[0m in \u001b[0;36mquick_execute\u001b[0;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[1;32m     51\u001b[0m   \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     52\u001b[0m     \u001b[0mctx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mensure_initialized\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 53\u001b[0;31m     tensors = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name,\n\u001b[0m\u001b[1;32m     54\u001b[0m                                         inputs, attrs, num_outputs)\n\u001b[1;32m     55\u001b[0m   \u001b[0;32mexcept\u001b[0m \u001b[0mcore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_NotOkStatusException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Step 4: Model Evaluation**\n",
        "\n",
        "\n",
        "\n",
        "Visualize Training Progress\n",
        "\n",
        "Plot accuracy and loss curves for training and validation data.\n",
        "\n",
        "Check Confusion Matrix\n",
        "\n",
        "See which classes are being misclassified.\n",
        "\n",
        "Generate Classification Report\n",
        "\n",
        "Get precision, recall, F1-score for each class."
      ],
      "metadata": {
        "id": "u0wncyTaT0nU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "X_test, y_test = preprocess_data(test_df, \"/content/GTSRB/\")\n",
        "y_test_categorical = to_categorical(y_test, num_classes=num_classes) # Preprocess test data\n",
        "\n",
        "# Evaluate on test data\n",
        "test_loss, test_accuracy = model.evaluate(X_test, y_test_categorical)\n",
        "print(f\"Test Accuracy: {test_accuracy:.4f}\")\n",
        "\n",
        "# Make predictions\n",
        "y_pred = model.predict(X_test)\n",
        "y_pred_classes = np.argmax(y_pred, axis=1)\n",
        "\n",
        "# Classification report\n",
        "print(\"\\nClassification Report:\")\n",
        "print(classification_report(y_test, y_pred_classes))\n",
        "\n",
        "# Confusion Matrix\n",
        "plt.figure(figsize=(15,12))\n",
        "cm = confusion_matrix(y_test, y_pred_classes)\n",
        "sns.heatmap(cm, annot=False, cmap='Blues')\n",
        "plt.title('Confusion Matrix - Test Data')\n",
        "plt.xlabel('Predicted Class')\n",
        "plt.ylabel('True Class')\n",
        "plt.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 655
        },
        "id": "slszyj4XT7Np",
        "outputId": "deeb3d64-f097-47c2-847d-a6834caa6536"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "name 'history' is not defined",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-2690070137.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msubplot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 10\u001b[0;31m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mplot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhistory\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhistory\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'accuracy'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'Train Accuracy'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     11\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mplot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhistory\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhistory\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'val_accuracy'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'Validation Accuracy'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtitle\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Accuracy over Epochs'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'history' is not defined"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 1200x500 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Step 6: Save the Model**"
      ],
      "metadata": {
        "id": "g2PGrvurUiYr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Save the trained model in HDF5 format\n",
        "model.save('traffic_sign_classifier.h5')\n",
        "# Optional: Save the model architecture as JSON\n",
        "model_json = model.to_json()\n",
        "with open(\"traffic_sign_classifier.json\", \"w\") as json_file:\n",
        "    json_file.write(model_json)\n",
        "\n",
        "print(\"Model saved successfully!\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vcjMwmyhUGjL",
        "outputId": "cc052927-db8d-4a11-89d1-107c239ab6ba"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model saved successfully!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Test the Model on New Images**"
      ],
      "metadata": {
        "id": "WCU6MumRlLwd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow.keras.models import load_model\n",
        "import cv2\n",
        "import numpy as np\n",
        "\n",
        "# Load the saved model\n",
        "model = load_model('traffic_sign_classifier.h5')\n",
        "\n",
        "# Load a new image\n",
        "img_path = '/content/GTSRB/Test/00195.png'\n",
        "img = cv2.imread(img_path)\n",
        "img = cv2.resize(img, (64,64))\n",
        "img = img.astype('float32') / 255.0\n",
        "img = np.expand_dims(img, axis=0)  # add batch dimension\n",
        "\n",
        "# Predict a single image\n",
        "pred = model.predict(img)\n",
        "class_idx = np.argmax(pred)\n",
        "print(\"Predicted Class:\", class_idx)\n",
        "\n"
      ],
      "metadata": {
        "id": "nkB63fhblNvw",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c11c8e12-0727-4b16-9773-a94f9e8352b1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 240ms/step\n",
            "Predicted Class: 12\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Conclusion**\n",
        "\n",
        "\n",
        "This project successfully built and evaluated a deep-learning model for traffic sign classification. By using a well-structured dataset, applying data preprocessing, and training a CNN, the model learned to recognize multiple traffic sign categories with strong accuracy. The results show that deep learning can reliably support real-time road-safety applications such as driver-assistance systems and autonomous vehicles. Future improvements could include experimenting with transfer learning, expanding the dataset with more real-world images, and optimizing the model for deployment on edge devices.\n"
      ],
      "metadata": {
        "id": "cu_jiYAgtLWy"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "N6gnfP_mvSnN"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}